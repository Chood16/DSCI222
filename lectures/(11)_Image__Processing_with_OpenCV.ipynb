{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chood16/DSCI222/blob/main/lectures/(11)_Image__Processing_with_OpenCV.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5dce0e02",
      "metadata": {
        "id": "5dce0e02"
      },
      "source": [
        "# Image Processing with OpenCV"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14f161b6",
      "metadata": {
        "id": "14f161b6"
      },
      "source": [
        "## 1. Image Basics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae56a98a",
      "metadata": {
        "id": "ae56a98a"
      },
      "outputs": [],
      "source": [
        "import cv2  # For Image Processing\n",
        "import numpy as np # Numpy\n",
        "from google.colab.patches import cv2_imshow # For Google Colab\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "arr = np.array([\n",
        "    [[100, 0, 0],   [0, 250, 0]],\n",
        "    [[0, 0, 255],  [150, 50, 125]]\n",
        "], dtype=np.uint8)\n",
        "\n",
        "# To display the image\n",
        "\n",
        "# *** The normal approach\n",
        "#cv2.imshow('image', image_orig)  # The normal approach\n",
        "#cv2.waitKey() # This is necessary to be required so that the image doesn't close immediately.\n",
        "#It will run continuously until the key press.\n",
        "#cv2.destroyAllWindows()\n",
        "\n",
        "# *** For Google Colab\n",
        "cv2_imshow(arr)\n"
      ],
      "metadata": {
        "id": "55NsHhDtHgpy"
      },
      "id": "55NsHhDtHgpy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigger = cv2.resize(arr, (200, 200))\n",
        "\n",
        "cv2_imshow(bigger)"
      ],
      "metadata": {
        "id": "uqdAERnHJ32K"
      },
      "id": "uqdAERnHJ32K",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigger = cv2.resize(arr, (200, 200), interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "cv2_imshow(bigger)"
      ],
      "metadata": {
        "id": "9rWPdRUDKXLx"
      },
      "id": "9rWPdRUDKXLx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To show some information about the image\n",
        "print('Type of object: ')\n",
        "display(type(bigger))\n",
        "\n",
        "print('\\nShape of the object/image: ')\n",
        "display( bigger.shape )\n"
      ],
      "metadata": {
        "id": "XwgrjH_gK4EP"
      },
      "id": "XwgrjH_gK4EP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What are the maximum and minimum RGB contributions?\n",
        "\n",
        "`arr = np.array([\n",
        "    [[100, 0, 0],   [0, 250, 0]],\n",
        "    [[0, 0, 255],  [150, 50, 125]]\n",
        "], dtype=np.uint8)`"
      ],
      "metadata": {
        "id": "u-WhxHCZLJ5a"
      },
      "id": "u-WhxHCZLJ5a"
    },
    {
      "cell_type": "code",
      "source": [
        "print(arr.max())\n"
      ],
      "metadata": {
        "id": "_Y-F8MuhJmql"
      },
      "id": "_Y-F8MuhJmql",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Collapsing rows\n",
        "print(arr.max(axis=0))\n",
        "\n"
      ],
      "metadata": {
        "id": "p6mC_7-xJEeb"
      },
      "id": "p6mC_7-xJEeb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Collapsing columns\n",
        "print(arr.max(axis=1))\n"
      ],
      "metadata": {
        "id": "OnW5ACDtJNUq"
      },
      "id": "OnW5ACDtJNUq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Collapsing rows and columns\n",
        "print(arr.max(axis=(0,1)))\n"
      ],
      "metadata": {
        "id": "UNXnHJTLJQ9m"
      },
      "id": "UNXnHJTLJQ9m",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For grayscale\n",
        "print('\\nMinimum value: ')\n",
        "display( np.min(bigger) )\n",
        "print('\\nMaximum value: ')\n",
        "display( np.max(bigger) )"
      ],
      "metadata": {
        "id": "jbkUcbQULCq5"
      },
      "id": "jbkUcbQULCq5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's go back, are the results what we expected?"
      ],
      "metadata": {
        "id": "WH1rb-sNMBX3"
      },
      "id": "WH1rb-sNMBX3"
    },
    {
      "cell_type": "code",
      "source": [
        "arr = np.array([\n",
        "    [[100, 0, 0],   [0, 250, 0]],\n",
        "    [[0, 0, 255],  [150, 50, 125]]\n",
        "], dtype=np.uint8)\n",
        "bigger = cv2.resize(arr, (200, 200), interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "cv2_imshow(bigger)\n"
      ],
      "metadata": {
        "id": "GYse-R8AMP_E"
      },
      "id": "GYse-R8AMP_E",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "982f930a",
      "metadata": {
        "id": "982f930a"
      },
      "outputs": [],
      "source": [
        "\n",
        "#https://docs.opencv.org/4.x/d4/da8/group__imgcodecs.html\n",
        "\n",
        "image_rgb = cv2.cvtColor(bigger, cv2.COLOR_BGR2RGB)\n",
        "cv2_imshow( image_rgb )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6abfdcf",
      "metadata": {
        "id": "c6abfdcf"
      },
      "source": [
        "## 2. Loading and Displaying Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc976b2e",
      "metadata": {
        "id": "cc976b2e"
      },
      "outputs": [],
      "source": [
        "import cv2  # For Image Processing\n",
        "import numpy as np # Numpy\n",
        "from google.colab.patches import cv2_imshow # For Google Colab\n",
        "path_img = \"Stadium.jpg\"\n",
        "\n",
        "# Using imread('path') and 1 denotes read as  color image\n",
        "image_orig = cv2.imread(path_img, 1)\n",
        "\n",
        "cv2_imshow(image_orig)\n",
        "\n",
        "# To open the original image as gray scale\n",
        "image_orig_gray = cv2.imread(path_img, 0)\n",
        "cv2_imshow(image_orig_gray)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('\\nShape of the object/image: ')\n",
        "display(image_orig.shape)\n",
        "\n",
        "# (rows, columns, channels)"
      ],
      "metadata": {
        "id": "E-4_RUKZNMSB"
      },
      "id": "E-4_RUKZNMSB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(image_orig)"
      ],
      "metadata": {
        "id": "POslVxDZH9t0"
      },
      "id": "POslVxDZH9t0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_rgb = cv2.cvtColor(image_orig, cv2.COLOR_BGR2RGB)\n",
        "cv2_imshow( image_rgb )"
      ],
      "metadata": {
        "id": "tEJ5snwDMyPk"
      },
      "id": "tEJ5snwDMyPk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7975a029",
      "metadata": {
        "id": "7975a029"
      },
      "outputs": [],
      "source": [
        "# To show each channel\n",
        "\n",
        "# B\n",
        "image_cpy = image_orig.copy()\n",
        "image_cpy[:,:,[1,2]] = 0 # To keep B\n",
        "cv2_imshow(image_cpy)\n",
        "\n",
        "# G\n",
        "image_cpy = image_orig.copy()\n",
        "image_cpy[:,:,[0,2]] = 0 # To keep G\n",
        "cv2_imshow(image_cpy)\n",
        "\n",
        "# R\n",
        "image_cpy = image_orig.copy()\n",
        "image_cpy[:,:,[0,1]] = 0 # To keep R\n",
        "cv2_imshow(image_cpy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3. Affine Transformation\n",
        "\n",
        "Take x,y coordinate --> Do Math --> get new x',y' coordinates\n",
        "\n",
        "https://docs.opencv.org/3.4/d4/d61/tutorial_warp_affine.html"
      ],
      "metadata": {
        "id": "ahtTBiTHR_0A"
      },
      "id": "ahtTBiTHR_0A"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b2db502",
      "metadata": {
        "id": "7b2db502"
      },
      "outputs": [],
      "source": [
        "# Get image height, width\n",
        "(h, w) = image_orig.shape[:2]\n",
        "# To compute the center of the image\n",
        "center = (w / 2, h / 2)\n",
        "\n",
        "print('Center of the image:')\n",
        "display(center)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# What is this M output?\n",
        "M = cv2.getRotationMatrix2D(center, angle=45, scale=.6)\n",
        "print(M)\n"
      ],
      "metadata": {
        "id": "MDp_f0FDKOTA"
      },
      "id": "MDp_f0FDKOTA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function returns a $2 \\times 3$ affine transformation matrix:\n",
        "\n",
        "$M =\n",
        "\\begin{bmatrix}\n",
        "\\alpha & \\beta & (1-\\alpha)\\cdot center_x - \\beta \\cdot center_y \\\\\n",
        "-\\beta & \\alpha & \\beta \\cdot center_x + (1-\\alpha)\\cdot center_y\n",
        "\\end{bmatrix}$\n",
        "\n",
        "where\n",
        "\n",
        "$\\alpha = scale \\cdot \\cos(\\theta)$,\n",
        "\n",
        "$\\beta = scale \\cdot \\sin(\\theta)$\n",
        "\n",
        "\n",
        "and $\\theta$ is the rotation angle.\n"
      ],
      "metadata": {
        "id": "EOXIeK0DLxoj"
      },
      "id": "EOXIeK0DLxoj"
    },
    {
      "cell_type": "code",
      "source": [
        "# How does warpAffine work?\n",
        "image_rotated45 = cv2.warpAffine(image_orig, M, (w, h))\n",
        "\n"
      ],
      "metadata": {
        "id": "ON-YQ0DOKXZj"
      },
      "id": "ON-YQ0DOKXZj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each output point $(x', y')$ is calculated from the input point $(x, y)$ using the affine matrix $M$:\n",
        "\n",
        "$\\begin{bmatrix}\n",
        "x' \\\\\n",
        "y'\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "m_{11} & m_{12} & m_{13} \\\\\n",
        "m_{21} & m_{22} & m_{23}\n",
        "\\end{bmatrix}\n",
        "\\cdot\n",
        "\\begin{bmatrix}\n",
        "x \\\\\n",
        "y \\\\\n",
        "1\n",
        "\\end{bmatrix}$\n",
        "\n",
        "where $M$ is the $2 \\times 3$ affine transformation matrix:\n",
        "\n",
        "$\n",
        "M =\n",
        "\\begin{bmatrix}\n",
        "m_{11} & m_{12} & m_{13} \\\\\n",
        "m_{21} & m_{22} & m_{23}\n",
        "\\end{bmatrix}\n",
        "$\n"
      ],
      "metadata": {
        "id": "8SyXYwKUOGyu"
      },
      "id": "8SyXYwKUOGyu"
    },
    {
      "cell_type": "code",
      "source": [
        "cv2_imshow(image_rotated45)"
      ],
      "metadata": {
        "id": "06uDx9gCUMAS"
      },
      "id": "06uDx9gCUMAS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Canny Edge Detector\n",
        "\n",
        "Used in Object Recognition\n",
        "\n",
        "https://en.wikipedia.org/wiki/Canny_edge_detector\n",
        "\n",
        "https://docs.opencv.org/4.x/da/d22/tutorial_py_canny.html"
      ],
      "metadata": {
        "id": "WDPyfp_CRt15"
      },
      "id": "WDPyfp_CRt15"
    },
    {
      "cell_type": "markdown",
      "source": [
        "How do we determine, where edges are?\n",
        "\n",
        "`threshold1` (lower threshold)\n",
        "\n",
        "* Pixels with gradient below this value are discarded.\n",
        "\n",
        "* Pixels with gradient between threshold1 and threshold2 are kept only if connected to a strong edge.\n",
        "\n",
        "* These are called weak edges.\n",
        "\n",
        "`threshold2` (upper threshold)\n",
        "\n",
        "* Pixels with gradient above this value are considered strong edges and always kept.\n",
        "\n",
        "Lower thresholds → detect more edges (even faint ones), but may include noise.\n",
        "\n",
        "Higher thresholds → detect only strong edges (less noise, but may miss subtle edges).\n",
        "\n",
        "`L2gradient`\n",
        "* `True` uses Euclidean Distance\n",
        "* `False` uses Taxicab Distance"
      ],
      "metadata": {
        "id": "VmguQfo9RKqF"
      },
      "id": "VmguQfo9RKqF"
    },
    {
      "cell_type": "code",
      "source": [
        "threshold1 = 200\n",
        "threshold2 = 300\n",
        "L2gradient = True\n",
        "image_edges = cv2.Canny(image_orig, threshold1, threshold2, L2gradient)\n",
        "cv2_imshow(image_edges)"
      ],
      "metadata": {
        "id": "RkYoh7BLRbOg"
      },
      "id": "RkYoh7BLRbOg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b886c4e",
      "metadata": {
        "id": "1b886c4e"
      },
      "outputs": [],
      "source": [
        "# This works for images of individuals too!\n",
        "path_img = 'Favorite_Professor.jpeg'\n",
        "\n",
        "# Using imread('path') and 1 denotes read as  color image\n",
        "image_face = cv2.imread(path_img, 1)\n",
        "\n",
        "# To display the new image\n",
        "cv2_imshow(image_face)\n",
        "\n",
        "# Canny Edge Detection\n",
        "image_edges_lady = cv2.Canny(image_face, 100, 200, True)\n",
        "cv2_imshow(image_edges_lady)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Bilateral Filter (Image Smoothing)\n",
        "https://docs.opencv.org/4.x/d4/d13/tutorial_py_filtering.html\n",
        "\n",
        "`Diameter`  (How big of a brush to use)\n",
        "* Represents pixel neighborhood size\n",
        "* Large value = large smoothing area\n",
        "\n",
        "`sigmaColor` (Amount of smoother)\n",
        "* Determines how different a pixel’s color can be to be averaged.\n",
        "* Small sigmaColor (e.g., 25–50): Only very similar colors are averaged → preserves more texture, less smoothing.\n",
        "* Large sigmaColor (e.g., 75–150): Larger differences are included → smoother skin, may wash out subtle features.\n",
        "\n",
        "`sigmaSpace` (Where smoothing occurs)\n",
        "* Determines how far in the image space the filter looks.\n",
        "* Small sigmaSpace (e.g., 25–50): Only nearby pixels influence averaging → fine detail preserved.\n",
        "* Large sigmaSpace (e.g., 75–150): Pixels farther away contribute → stronger smoothing over wider areas."
      ],
      "metadata": {
        "id": "CFcp5U6UXHKd"
      },
      "id": "CFcp5U6UXHKd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d424ba2c",
      "metadata": {
        "id": "d424ba2c"
      },
      "outputs": [],
      "source": [
        "cv2_imshow(image_face)\n",
        "\n",
        "# After bilateral filter\n",
        "d = 12\n",
        "sigmaColor = 55\n",
        "sigmaSpace = 55\n",
        "image_face_bilat = cv2.bilateralFilter(image_face, d, sigmaColor, sigmaSpace)\n",
        "cv2_imshow(image_face_bilat)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can blurr images too!"
      ],
      "metadata": {
        "id": "lds3GjXMixVy"
      },
      "id": "lds3GjXMixVy"
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply Gaussian blur directly on the color image\n",
        "kernal = (11,11) # the \"window size\" used for blurring each pixel\n",
        "SigmaX = 0 # how much close vs far neighbors impact each pixel.\n",
        "# 0 means we don't calculate it, OpenCV does\n",
        "\n",
        "blurred_color = cv2.GaussianBlur(image_face, kernal, SigmaX)\n",
        "\n",
        "# Display the blurred color image\n",
        "cv2_imshow(image_face)\n",
        "cv2_imshow(blurred_color)"
      ],
      "metadata": {
        "id": "jH73Yx-ZizQT"
      },
      "id": "jH73Yx-ZizQT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Image Matching"
      ],
      "metadata": {
        "id": "7hAvPK2JaoKL"
      },
      "id": "7hAvPK2JaoKL"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ddce2e7",
      "metadata": {
        "id": "9ddce2e7"
      },
      "outputs": [],
      "source": [
        "# Let's see if we can find he following person in the image\n",
        "\n",
        "# Original template\n",
        "i_spy = cv2.imread('Hidden_Flag.jpeg') # Flag, Person\n",
        "cv2_imshow(i_spy)\n",
        "\n",
        "# The method we will be using requires gray-scale images\n",
        "i_spy_gray = cv2.imread('Hidden_Flag.jpeg', 0)\n",
        "cv2_imshow(i_spy_gray)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Here is a reminder of the image we are searching\n",
        "\n",
        "# To display the original image\n",
        "cv2_imshow(image_orig)\n",
        "# To open the original image as gray scale\n",
        "path_img = \"Stadium.jpg\"\n",
        "image_orig_gray = cv2.imread(path_img, 0)\n",
        "cv2_imshow(image_orig_gray)"
      ],
      "metadata": {
        "id": "kMSQl-x1i4TG"
      },
      "id": "kMSQl-x1i4TG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How does matching in OpenCV work?\n",
        "\n",
        "`cv2.matchTemplate` slides the template image (`i_spy_gray`) over the source image (`image_orig_gray`) like a “window” and computes a similarity score at each position.\n",
        "\n",
        "Think of it as checking how well the template matches each part of the big image.\n",
        "\n",
        "The result is a 2D matrix res where each element corresponds to the matching score at a specific ***top-left coordinate*** in the source image.\n",
        "\n",
        "This can feel like facial recognition, but is not nearly as robust. The image in the target and source need to be almost identical"
      ],
      "metadata": {
        "id": "6Q6r46Rxcc-k"
      },
      "id": "6Q6r46Rxcc-k"
    },
    {
      "cell_type": "code",
      "source": [
        "# MATCHING\n",
        "# To Find the location of the template within the BIG image\n",
        "w, h = i_spy_gray.shape[::-1]\n",
        "# Apply template Matching\n",
        "res = cv2.matchTemplate(image_orig_gray, i_spy_gray, eval('cv2.TM_CCOEFF'))\n",
        "print(res)"
      ],
      "metadata": {
        "id": "cV8_2X9vbpDv"
      },
      "id": "cV8_2X9vbpDv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For the cv2.TM_CCOEFF evaluation methos, larger is better\n",
        "# If the method is TM_SQDIFF or TM_SQDIFF_NORMED, take minimum\n",
        "min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(res)\n",
        "\n",
        "# Let's make a red rectangle around the image that was found\n",
        "top_left = max_loc\n",
        "bottom_right = (top_left[0] + w, top_left[1] + h)\n",
        "color_rec = (0, 0, 255) # Remember: B G R order\n",
        "thickness = 3\n",
        "\n",
        "# Draw rectangle on image\n",
        "cv2.rectangle(image_orig, top_left, bottom_right, color_rec, thickness)\n",
        "cv2_imshow(image_orig)"
      ],
      "metadata": {
        "id": "nIm3r_twcJxO"
      },
      "id": "nIm3r_twcJxO",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}