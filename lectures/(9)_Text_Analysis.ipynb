{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chood16/DSCI222/blob/main/lectures/(9)_Text_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15f05e6d",
      "metadata": {
        "id": "15f05e6d"
      },
      "source": [
        "# Text Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "229c2bea",
      "metadata": {
        "id": "229c2bea"
      },
      "source": [
        "## Why This Matters\n",
        "\n",
        "* Text data powers search engines, chatbots, sentiment analysis, and more\n",
        "* Raw text is messy — preprocessing is essential for accuracy\n",
        "* Translation expands applications across languages and cultures"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "834cb9d8",
      "metadata": {
        "id": "834cb9d8"
      },
      "source": [
        "## The Text Analysis Pipeline\n",
        "\n",
        "1. **Collection** – get raw text (web, files, APIs)\n",
        "2. **Preprocessing** – clean and normalize\n",
        "3. **Feature Extraction** – convert to numerical form\n",
        "4. **Analysis** – classification, clustering, sentiment, etc.\n",
        "5. **Translation (if needed)** – convert between languages"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17d064bc",
      "metadata": {
        "id": "17d064bc"
      },
      "source": [
        "## Common Preprocessing Steps\n",
        "\n",
        "* Lowercasing\n",
        "* Removing punctuation, numbers, and stopwords\n",
        "* Tokenization (splitting into words/tokens)\n",
        "* Stemming & Lemmatization\n",
        "* Handling special characters & emojis\n",
        "* Language detection"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regular Expression (regex)\n",
        "\n"
      ],
      "metadata": {
        "id": "Apm1ul7a0ZYk"
      },
      "id": "Apm1ul7a0ZYk"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Metacharacters Cheat Sheet\n",
        "\n",
        "| Symbol  | Meaning                                                                |                                         |\n",
        "| ------- | ---------------------------------------------------------------------- | --------------------------------------- |\n",
        "| `.`     | Matches any character except newline (`\\n`)                            |                                         |\n",
        "| `^`     | Matches the start of a string (or line in multiline mode)              |                                         |\n",
        "| `$`     | Matches the end of a string (or line in multiline mode)                |                                         |\n",
        "| `*`     | Matches 0 or more repetitions of the preceding pattern                 |                                         |\n",
        "| `+`     | Matches 1 or more repetitions of the preceding pattern                 |                                         |\n",
        "| `?`     | Matches 0 or 1 repetition of the preceding pattern (makes it optional) |                                         |\n",
        "| `{m}`   | Matches exactly `m` repetitions                                        |                                         |\n",
        "| `{m,}`  | Matches `m` or more repetitions                                        |                                         |\n",
        "| `{m,n}` | Matches between `m` and `n` repetitions                                |                                         |\n",
        "| `[]`    | Matches any single character inside the brackets (character class)     |                                         |\n",
        "| `[^ ]`  | Matches any single character **not** inside the brackets               |                                         |\n",
        "| `[a-t]` | Matches lowercase letters a through t |\n",
        "| `[a-tB-P]` | Matches lowercase letters a through t or uppercase B-P|\n",
        "| `[0-4]` | Matches number 0 through 4\n",
        "|  `\\|`        | Acts as an OR operator between patterns |\n",
        "| `()`    | Groups patterns and captures the matched text                          |                                         |\n",
        "| `(?: )` | Groups patterns **without capturing** (non-capturing group)            |                                         |\n",
        "| `(?P<name>pattern)` | Create a named captured group|"
      ],
      "metadata": {
        "id": "0b4n40JD0eUu"
      },
      "id": "0b4n40JD0eUu"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Special Character Cheat Sheet\n",
        "\n",
        "| Symbol | Meaning                                                          |\n",
        "| ------ | ---------------------------------------------------------------- |\n",
        "| `\\d`   | Matches any digit (`0-9`)                                        |\n",
        "| `\\D`   | Matches any non-digit                                            |\n",
        "| `\\w`   | Matches any “word” character (letters, digits, underscore)       |\n",
        "| `\\W`   | Matches any non-word character                                   |\n",
        "| `\\s`   | Matches any whitespace (space, tab, newline)                     |\n",
        "| `\\S`   | Matches any non-whitespace                                       |\n",
        "| `\\b`   | Matches word boundary (between word and non-word)                |\n",
        "| `\\B`   | Matches position **not** at a word boundary                      |\n",
        "| `\\\\`   | Escapes a special character (e.g. `\\.` matches a literal period) |\n"
      ],
      "metadata": {
        "id": "8Ff4MNKY0o38"
      },
      "id": "8Ff4MNKY0o38"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some examples\n",
        "\n",
        "| Symbol | Example                              | Matches                   |\n",
        "| ------ | ------------------------------------ | ------------------------- |\n",
        "| `^`    | `re.findall(r\"^cat\", \"cat dog cat\")` | `['cat']` (only at start) |\n",
        "| `$`    | `re.findall(r\"cat$\", \"dog cat\")`     | `['cat']` (only at end)   |\n",
        "| `.`     | `re.findall(r\"c.t\", \"cat cot cut cit\")`  | `['cat','cot','cut','cit']` |\n",
        "| `*`     | `re.findall(r\"ab*\", \"a ab abb abbb\")`    | `['a','ab','abb','abbb']`   |\n",
        "| `+`     | `re.findall(r\"ab+\", \"a ab abb abbb\")`    | `['ab','abb','abbb']`       |\n",
        "| `?`     | `re.findall(r\"ab?\", \"a ab abb\")`         | `['a','ab','ab']`           |\n",
        "| `{m}`   | `re.findall(r\"a{3}\", \"aa aaa aaaa\")`     | `['aaa','aaa']`             |\n",
        "| `{m,}`  | `re.findall(r\"a{2,}\", \"a aa aaa aaaa\")`  | `['aa','aaa','aaaa']`       |\n",
        "| `{m,n}` | `re.findall(r\"a{2,3}\", \"a aa aaa aaaa\")` | `['aa','aaa','aaa']`        |\n",
        "| `[abc]`  | `re.findall(r\"[abc]\", \"apple bat cat\")` | `['a','b','a','c','a']` |\n",
        "| `[^abc]` | `re.findall(r\"[^abc]\", \"abcxyz\")`       | `['x','y','z']`         |\n",
        "| `[0-9]`  | `re.findall(r\"[0-9]\", \"Room 101\")`      | `['1','0','1']`         |\n",
        "| `(...)`   | `re.findall(r\"(cat)\", \"cat dog cat\")`   | `['cat','cat']`                    |                         |                 |\n",
        "| `(?:...)` | `re.findall(r\"(?:cat)\", \"cat dog cat\")` | `['cat','cat']` (no capture group) |                         |                 |\n",
        "| `\\| `     | `re.findall(r\"cat \\| dog\", \"cat dog bird\")` | `['cat','dog']` |\n",
        "| `\\d`   | `re.findall(r\"\\d\", \"A1B2C3\")`                   | `['1','2','3']`               |\n",
        "| `\\D`   | `re.findall(r\"\\D\", \"A1B2\")`                     | `['A','B']`                   |\n",
        "| `\\w`   | `re.findall(r\"\\w\", \"Hi_5!\")`                    | `['H','i','_','5']`           |\n",
        "| `\\W`   | `re.findall(r\"\\W\", \"Hi_5!\")`                    | `['!']`                       |\n",
        "| `\\s`   | `re.findall(r\"\\s\", \"a b\\tc\\n\")`                 | `[' ','\\t','\\n']`             |\n",
        "| `\\S`   | `re.findall(r\"\\S\", \"a b\")`                      | `['a','b']`                   |\n",
        "| `\\b`   | `re.findall(r\"\\bcat\\b\", \"a cat scatter cater\")` | `['cat']`                     |\n",
        "| `\\B`   | `re.findall(r\"\\Bcat\\B\", \"scat cat scatter\")`    | `['cat']` (inside words only) |\n",
        "| `(?=...)`  | `re.findall(r\"cat(?=dog)\", \"catdog catfish\")`           | `['cat']`             |\n",
        "| `(?!...)`  | `re.findall(r\"cat(?!dog)\", \"catdog catfish\")`           | `['cat']`             |\n",
        "| `(?<=...)` | `re.findall(r\"(?<=Mr\\.)\\s\\w+\", \"Mr. Smith, Mr. Jones\")` | `[' Smith',' Jones']` |\n",
        "| `(?<!...)` | `re.findall(r\"(?<!Mr\\.)\\s\\w+\", \"Mr. Smith Ms. Taylor\")` | `[' Taylor']`         |\n",
        "| `\\.`   | `re.findall(r\"\\.\", \"a.b c.d\")`   | `['.','.']` |\n",
        "| `\\\\`   | `re.findall(r\"\\\\\", r\"a\\b c\\\\d\")` | `['\\\\']`    |\n",
        "\n"
      ],
      "metadata": {
        "id": "_xdXluaC1tgf"
      },
      "id": "_xdXluaC1tgf"
    },
    {
      "cell_type": "markdown",
      "id": "83973899",
      "metadata": {
        "id": "83973899"
      },
      "source": [
        "## Example: Basic Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bee11ac2",
      "metadata": {
        "id": "bee11ac2"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "text = \"Hello World! 123 :)\"\n",
        "\n",
        "# a-zA-Z: match any letter\n",
        "# \\s: match whitespace\n",
        "# ^: negates the expression\n",
        "# \"\": replace with nothing\n",
        "# Together, this takes all non-letters, and replace them with nothing, then lowercase\n",
        "cleaned = re.sub(r\"[^a-zA-Z\\s]\", \"\", text).lower()\n",
        "print(cleaned)  # \"hello world\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's practice!\n",
        "\n",
        "Here is a reminder of what we already know\n",
        "\n",
        "| Method      | Purpose                                   | Example                                                      | Output                  |\n",
        "|------------|-------------------------------------------|-------------------------------------------------------------|------------------------|\n",
        "| `match()`   | Check pattern at **start** of string     | `re.match(r\"\\d+\", \"123abc\")`                                | Match object (matches '123') |\n",
        "| `search()`  | Find pattern **anywhere** in string      | `re.search(r\"\\d+\", \"abc123\")`                               | Match object (matches '123') |\n",
        "| `findall()` | Find **all matches** in string           | `re.findall(r\"\\d+\", \"I have 2 cats and 3 dogs\")`           | `['2', '3']`           |\n",
        "| `sub()`     | **Replace matches** with another string  | `re.sub(r\"\\d\", \"#\", \"123-456\")`                             | `\"###-###\"`            |\n"
      ],
      "metadata": {
        "id": "j2qTMrZ-6Ywt"
      },
      "id": "j2qTMrZ-6Ywt"
    },
    {
      "cell_type": "code",
      "source": [
        "voicemail = \"\"\"Hi, this is Alice. I’m calling about order A123.\n",
        "I placed it on 2025-09-17, and I think the price was $19.99.\n",
        "My coworker Bob also ordered, his ID was B456 on 2024-12-01, and it cost about $250.00.\n",
        "If you need to reach me, my email is alice@example.com.\n",
        "Bob’s email is bob.smith@school.edu.\n",
        "Oh, and my friend Carol had order C789 on 1999-07-04 for only $7.5. Thanks!\"\"\"\n"
      ],
      "metadata": {
        "id": "Og3IB-Ds5oa4"
      },
      "id": "Og3IB-Ds5oa4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tasks:\n",
        "\n",
        "* Write a regex to find all the order IDs (like A123, B456).\n",
        "\n",
        "* How would you match all the dates in YYYY-MM-DD format?\n",
        "\n",
        "* How can you extract just the letters from the order IDs?\n",
        "\n",
        "* How can you extract just the numbers from the order IDs?\n",
        "\n",
        "* Write a regex that matches four-digit years in the dates.\n",
        "\n",
        "* Modify it so it only matches years starting with 202.\n",
        "\n",
        "* Create a regex that extracts the username and domain separately from each email (e.g., alice and example.com).\n",
        "\n",
        "* How would you capture the month part of the dates?\n",
        "\n",
        "* Write a regex that finds all the prices (including decimals).\n",
        "\n",
        "* How would you anonymize all email addresses in the voicemail by replacing them with [EMAIL REDACTED]?\n",
        "\n",
        "* Replace all order IDs (e.g., A123, B456) with the word ORDER.\n",
        "\n",
        "* Check to see if the voicemail includes an email address.\n",
        "\n",
        "* Convert each date in YYYY-MM-DD form to DD-MM-YY\n"
      ],
      "metadata": {
        "id": "jHh6UI2O6hvk"
      },
      "id": "jHh6UI2O6hvk"
    },
    {
      "cell_type": "code",
      "source": [
        "ans1 = re.findall(r\"[A-Z]\\d{3}\", voicemail)\n",
        "ans2 = re.findall(r\"\\d{4}-(\\d{2})-\\d{2}\", voicemail)\n",
        "ans3 = re.findall(r\"[A-Z](\\d{3})\", voicemail)\n",
        "ans4 = re.sub(r\"[A-Z]\\d{3}\", \"ORDER\", voicemail)\n",
        "ans5 = re.findall(r\"\\b202\\d\\b\", voicemail)\n",
        "ans6 = re.findall(r\"\\b\\d{4}\\b\", voicemail)\n",
        "ans7 = re.findall(r\"([\\w\\.\\-]+)@([\\w\\.\\-]+\\.\\w+)\", voicemail)\n",
        "ans8 = re.findall(r\"\\$\\d+(?:\\.\\d+)?\", voicemail)\n",
        "ans9 = re.sub(r\"[\\w\\.\\-]+@[\\w\\.\\-]+\\.\\w+\", \"[EMAIL REDACTED]\", voicemail)\n",
        "ans10 = re.search(r\"[\\w\\.\\-]+@[\\w\\.\\-]+\\.\\w+\", voicemail)\n",
        "ans11 = re.findall(r\"([A-Z])\\d{3}\", voicemail)\n",
        "ans12 = re.findall(r\"\\d{4}-\\d{2}-\\d{2}\", voicemail)\n",
        "ans13 = re.sub(r\"(\\d{4})-(\\d{2})-(\\d{2})\", lambda m: f\"{m.group(3)}-{m.group(2)}-{m.group(1)[2:]}\", voicemail)"
      ],
      "metadata": {
        "id": "6nH_o377AMfD"
      },
      "id": "6nH_o377AMfD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "4f33d62e",
      "metadata": {
        "id": "4f33d62e"
      },
      "source": [
        "# Tokenization\n",
        "\n",
        "* Breaking text into words, sentences, characters, subwords, etc.\n",
        "* [SentencePiece](https://arxiv.org/abs/1808.06226)\n",
        "* [WordPiece](https://arxiv.org/abs/2012.15524)\n",
        "* [byte-level BPE (Byte Pair Encoding)](https://arxiv.org/abs/1909.03341)\n",
        "\n",
        "| Model Family         | Example Models                 | Tokenization Method  |\n",
        "| -------------------- | ------------------------------ | -------------------- |\n",
        "| **BERT family**      | BERT, DistilBERT, ELECTRA      | WordPiece            |\n",
        "| **RoBERTa family**   | RoBERTa, XLM-R                 | BPE (non-byte)       |\n",
        "| **Google Seq2Seq**   | T5, mT5, XLNet, ALBERT         | SentencePiece        |\n",
        "| **OpenAI GPT**       | GPT-2, GPT-3, GPT-4, CLIP      | Byte-level BPE       |\n",
        "| **Meta**             | OPT (byte-BPE), LLaMA (SP-BPE) | BPE / SentencePiece  |\n",
        "| **Anthropic Claude** | Claude 1–3                     | Byte-level BPE       |\n",
        "| **Mistral**          | Mistral, Mixtral               | SentencePiece BPE    |\n",
        "| **BigScience**       | BLOOM                          | Byte-level BPE       |\n",
        "| **Microsoft**        | DeBERTa (WordPiece), Phi (BPE) | WordPiece / Byte-BPE |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization with regex"
      ],
      "metadata": {
        "id": "02IEdDi8Mlrw"
      },
      "id": "02IEdDi8Mlrw"
    },
    {
      "cell_type": "code",
      "source": [
        "# Why do we need to learn all of this?\n",
        "# Why can't we just use .split?\n",
        "\n",
        "text = \"Data Science is fun! Once you learn it, you will never forget it\"\n",
        "tokens = text.split()\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "c2DoWmwW5Jyb"
      },
      "id": "c2DoWmwW5Jyb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b51eedfd",
      "metadata": {
        "id": "b51eedfd"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# \\w+: match any word characters (letters, numbers, underscore) with repetition\n",
        "text = \"Data Science is fun! Once you learn it, you will never forget it\"\n",
        "text = text.lower()\n",
        "tokens = re.findall(r\"\\w+\", text)\n",
        "characters = re.findall(r\"[^\\w\\s]\", text)\n",
        "print(tokens)\n",
        "print(characters)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization can get tricky quickly\n",
        "# What are some ways we can hand'le It's?\n",
        "\n",
        "text = \"I don't think it's going to get tough quick, do you?\"\n",
        "text = text.lower()\n",
        "tokens = re.findall(r\"\\w+\", text) # <-- \\w is the same as [a-zA-Z0-9_]\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "PqSlwSYV72fq"
      },
      "id": "PqSlwSYV72fq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "text = \"I don't think it's going to get tough quick, do you?\"\n",
        "text = text.lower()\n",
        "tokens = re.findall(r\"[\\w']+\", text) # <-- captures repetitions of both word characters and apostrophe\n",
        "\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "dgGjFGWH9B2Z"
      },
      "id": "dgGjFGWH9B2Z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We could do a substitution manually for each contraction\n",
        "\n",
        "text = \"I don't think it's going to get tough quick, do you?\"\n",
        "text = text.lower()\n",
        "text = re.sub(r\"\\bit's\\b\", \"it is\", text)\n",
        "text = re.sub(r\"\\bdon't\\b\", \"do not\", text)\n",
        "tokens = re.findall(r\"[\\w']+\", text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "rVieIa6VJJe6"
      },
      "id": "rVieIa6VJJe6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We could also create a dictionary of contractions\n",
        "\n",
        "text = text = \"I don't think it's going to get tough quick, do you?\"\n",
        "text = text.lower()\n",
        "\n",
        "contractions_dict = {\n",
        "    \"don't\": \"do not\",\n",
        "    \"it's\": \"it is\",\n",
        "    \"we're\": \"we are\"\n",
        "}\n",
        "\n",
        "# Then make a list of the contractions\n",
        "key_list = list(contractions_dict.keys())\n",
        "\n",
        "# Join the strings in the list, using | as the separator\n",
        "pattern_text = \"|\".join(key_list)\n",
        "\n",
        "# Insert this separator into a pattern\n",
        "pattern = r\"\\b(?:{})\\b\".format(pattern_text)\n",
        "\n",
        "# We can then search for our pattern\n",
        "# For each match found, match.group() is the contraction found in the text\n",
        "# We are then finding the value in our dictionary associated with this match\n",
        "# Finally, we sub out the match with the value of the match key\n",
        "expanded_text = re.sub(pattern, lambda match: contractions_dict[match.group()],text)\n",
        "\n",
        "print(expanded_text)\n"
      ],
      "metadata": {
        "id": "Xk_NrhpnEWi8"
      },
      "id": "Xk_NrhpnEWi8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions"
      ],
      "metadata": {
        "id": "x-CBdxC6OTB8"
      },
      "id": "x-CBdxC6OTB8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# There is a python package called contractions that does this for us!\n",
        "\n",
        "import contractions\n",
        "import re\n",
        "text = text = \"I don't think it's going to get tough quick, do you?\"\n",
        "text = text.lower()\n",
        "\n",
        "# Expand contractions automatically\n",
        "text = contractions.fix(text)\n",
        "\n",
        "print(text)\n"
      ],
      "metadata": {
        "id": "5haEB6bmOVXG"
      },
      "id": "5haEB6bmOVXG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = re.findall(r\"[\\w']+\", text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "2sqoJ3twMc9t"
      },
      "id": "2sqoJ3twMc9t",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization Using NLTK (Natural Language Toolkit)"
      ],
      "metadata": {
        "id": "bdvxZV_5Mpyu"
      },
      "id": "bdvxZV_5Mpyu"
    },
    {
      "cell_type": "markdown",
      "source": [
        "This may not be the best way to split up our contractions and utilize tokenization.\n",
        "\n",
        "We are missing cruicial contributors, such as punctuation\n",
        "\n",
        "`word_tokenize` is a tokenizer by nltk\n",
        "\n",
        "* It's like using a regex such as `[\\w']+` to split text into words.\n",
        "\n",
        "* The key difference is that `word_tokenize` uses the `Punkt` tokenizer under the hood, which is language-aware:\n",
        "\n",
        "* Handles contractions (can't → 'ca' + \"n't\")\n",
        "\n",
        "* Separates punctuation as individual tokens\n",
        "\n",
        "* Deals with abbreviations (Dr., Mr., etc.)\n",
        "\n",
        "* Regex like [\\w']+ is simpler, but won’t handle all the linguistic nuances."
      ],
      "metadata": {
        "id": "cECVJaLONnle"
      },
      "id": "cECVJaLONnle"
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download required NLTK data (only the first time)\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vuG3l5PfOGsa"
      },
      "id": "vuG3l5PfOGsa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I don't think it's going to get tough quick, do you?\"\n",
        "text = text.lower()\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)\n"
      ],
      "metadata": {
        "id": "XP6aT75nOIKN"
      },
      "id": "XP6aT75nOIKN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "5185570d",
      "metadata": {
        "id": "5185570d"
      },
      "source": [
        "### Stopword Removal\n",
        "\n",
        "* Removes common but uninformative words (e.g., “the”, “and”)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "ixYPoPXuCN10"
      },
      "id": "ixYPoPXuCN10",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "fIbBSRJ1PYfL"
      },
      "id": "fIbBSRJ1PYfL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I don't think it's going to get tough quick, do you?\"\n",
        "text = text.lower()\n",
        "\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Remove stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokens = [t for t in tokens if t not in stop_words]\n",
        "\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "lk05aWp_PMWz"
      },
      "id": "lk05aWp_PMWz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "46483a51",
      "metadata": {
        "id": "46483a51"
      },
      "source": [
        "### More fun with NLTK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b58ca403",
      "metadata": {
        "id": "b58ca403"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "nltk.download('wordnet')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* WordNet is a large lexical database of English.\n",
        "\n",
        "* Words can be stemmed\n",
        "\n",
        "* Words can be lemmatized"
      ],
      "metadata": {
        "id": "VdclBTFGR-dF"
      },
      "id": "VdclBTFGR-dF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming: the process of reducing a word to its root or base form, called a stem."
      ],
      "metadata": {
        "id": "NGHIiRqnTnMC"
      },
      "id": "NGHIiRqnTnMC"
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize stemmer\n",
        "ps = PorterStemmer()\n",
        "\n",
        "# What issues do we see?\n",
        "print(ps.stem(\"running\"))\n",
        "print(ps.stem(\"flies\"))\n",
        "print(ps.stem(\"easily\"))\n",
        "print(ps.stem(\"better\"))\n",
        "print(ps.stem(\"studies\"))\n",
        "print(ps.stem(\"organizational\"))\n",
        "print(ps.stem(\"unbelievable\"))\n",
        "print(ps.stem(\"geese\"))\n",
        "print(ps.stem(\"running-away\"))\n",
        "print(ps.stem(\"happily\"))"
      ],
      "metadata": {
        "id": "nSHJP2gpW3CM"
      },
      "id": "nSHJP2gpW3CM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatization: the process of reducing a word to its dictionary form"
      ],
      "metadata": {
        "id": "XZFGSOVfUcV2"
      },
      "id": "XZFGSOVfUcV2"
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize lemmatizer\n",
        "lem = WordNetLemmatizer()\n",
        "words_with_pos = [\n",
        "    (\"running\", \"v\"),\n",
        "    (\"flies\", \"v\"),\n",
        "    (\"easily\", \"r\"),  # adverb\n",
        "    (\"better\", \"a\"),  # adjective\n",
        "    (\"studies\", \"n\"),\n",
        "    (\"organizational\", \"a\"),\n",
        "    (\"unbelievable\", \"a\"),\n",
        "    (\"geese\", \"n\"),\n",
        "    (\"running-away\", \"v\"),\n",
        "    (\"happily\", \"r\")\n",
        "]\n",
        "\n",
        "print(f\"{'Word':<15} {'Stemmed':<15} {'Lemmatized':<15}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for word, pos in words_with_pos:\n",
        "    stemmed = ps.stem(word)\n",
        "    lemmatized = lem.lemmatize(word, pos=pos)\n",
        "    notes = \"\"\n",
        "    print(f\"{word:<15} {stemmed:<15} {lemmatized:<15}\")\n"
      ],
      "metadata": {
        "id": "PLp9MHaMP0XS"
      },
      "id": "PLp9MHaMP0XS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What does tokenization look like for different subword tokenization methods?"
      ],
      "metadata": {
        "id": "qmpEm_p7cnjv"
      },
      "id": "qmpEm_p7cnjv"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, GPT2Tokenizer, T5Tokenizer\n",
        "import sentencepiece as spm\n",
        "\n",
        "text = \"I don't think it's going to get tough quick, do you?\"\n",
        "\n",
        "# 1. SentencePiece\n",
        "# For demo purposes, we'll use a pretrained SentencePiece model (t5-small)\n",
        "# We use from_pretrained for tokenizers because the tokenization rules and vocabularies are learned during model training.\n",
        "\n",
        "sp_tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "sp_tokens = sp_tokenizer.tokenize(text)\n",
        "print(\"SentencePiece tokens:\", sp_tokens)\n",
        "\n",
        "# The underscore ▁ in SentencePiece tokens is a special marker that indicates a word boundary"
      ],
      "metadata": {
        "id": "BB9SeKIHQux0"
      },
      "id": "BB9SeKIHQux0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. WordPiece (BERT)\n",
        "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "wp_tokens = bert_tokenizer.tokenize(text)\n",
        "print(\"WordPiece tokens:\", wp_tokens)"
      ],
      "metadata": {
        "id": "Wk-xcltiRhLq"
      },
      "id": "Wk-xcltiRhLq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Byte-level BPE (GPT-2)\n",
        "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "bpe_tokens = gpt2_tokenizer.tokenize(text)\n",
        "print(\"Byte-level BPE tokens:\", bpe_tokens)\n",
        "\n",
        "# spaces are included in the tokens and are represented by Ġ"
      ],
      "metadata": {
        "id": "6cftULu3RoTj"
      },
      "id": "6cftULu3RoTj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's highlight some additional differences between these three methods of tokenization\n",
        "text = \"Unbelievable! It's going to rock the AI-world 🚀.\"\n",
        "sp_tokens = sp_tokenizer.tokenize(text)\n",
        "print(\"SentencePiece tokens:\", sp_tokens)\n",
        "\n",
        "wp_tokens = bert_tokenizer.tokenize(text)\n",
        "print(\"WordPiece tokens:\", wp_tokens)\n",
        "\n",
        "bpe_tokens = gpt2_tokenizer.tokenize(text)\n",
        "print(\"Byte-level BPE tokens:\", bpe_tokens)\n"
      ],
      "metadata": {
        "id": "g5s2NqtRQHJQ"
      },
      "id": "g5s2NqtRQHJQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"oivaohuqonfmoimw surely isn't a word\"\n",
        "sp_tokens = sp_tokenizer.tokenize(text)\n",
        "print(\"SentencePiece tokens:\", sp_tokens)\n",
        "\n",
        "wp_tokens = bert_tokenizer.tokenize(text)\n",
        "print(\"WordPiece tokens:\", wp_tokens)\n",
        "\n",
        "bpe_tokens = gpt2_tokenizer.tokenize(text)\n",
        "print(\"Byte-level BPE tokens:\", bpe_tokens)"
      ],
      "metadata": {
        "id": "meq8l03fU5rs"
      },
      "id": "meq8l03fU5rs",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "02IEdDi8Mlrw"
      ],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}